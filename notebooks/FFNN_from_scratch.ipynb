{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7541a0f-92d5-4f5a-9d8d-f07c3e275d29",
   "metadata": {},
   "source": [
    "# Train a neural network to predict MHC ligands\n",
    "The notebook consists of the following sections:\n",
    "\n",
    "0. Module imports, define functions, set constants\n",
    "1. Load Data\n",
    "2. Build Model\n",
    "3. Select Hyper-paramerters\n",
    "4. Compile Model\n",
    "5. Train Model\n",
    "6. Evaluation\n",
    "\n",
    "## Exercise\n",
    "\n",
    "The exercise is to optimize the model given in this notebook by selecting hyper-parameters that improve performance. First run the notebook as is and take notes of the performance (AUC, MCC). Then start a manual hyper-parameter search by following the instructions below. If your first run results in poor fitting (the model doesn't learn anything during training) do not dispair! Hopefully you will see a rapid improvement when you start testing other hyper-parameters.\n",
    "\n",
    "### Optimizer, learning rate, and mini-batches\n",
    "The [optimizers](https://pytorch.org/docs/stable/optim.html) are different approaches of minimizing a loss function based on gradients. The learning rate determine to which degree we correct the weights. The smaller the learning rate, the smaller corrections we make. This may prolong the training time. To mitigate this, one can train with mini-batches. Instead of feeding your network all of the data before you make updates you can partition the training data into mini-batches and update weigths more frequently. Thus, your model might converge faster. Also small batch sizes use less memory, which means you can train a model with more parameters.\n",
    "\n",
    "If you experienced trouble in even training then you might benefit from lowering the learning rate to 0.01 or 0.001 or perhaps even smaller.\n",
    "\n",
    "__Optimizers:__\n",
    "1. SGD (+ Momentum)\n",
    "2. Adam\n",
    "3. Try others if you like...\n",
    "\n",
    "__Mini-batch size:__\n",
    "When you have implemented and tested a smaller learning rate try also implementing a mini-batch of size 512 or 128. In order to set the mini-batch size use the variable MINI_BATCH_SIZE and run train_with_minibatches() instead of train().\n",
    "\n",
    "### Number of hidden units\n",
    "Try increasing the number of model parameters (weights), eg. 64, 128, or 512.\n",
    "\n",
    "### Hidden layers\n",
    "Add another layer to the network. To do so you must edit the methods of Net()-class.\n",
    "\n",
    "### Parameter initialization\n",
    "Parameter initialization can be extremely important.\n",
    "PyTorch has a lot of different [initializers](http://pytorch.org/docs/master/nn.html#torch-nn-init) and the most often used initializers are listed below. Try implementing one of them.\n",
    "1. Kaming He\n",
    "2. Xavier Glorot\n",
    "3. Uniform or Normal with small scale (0.1 - 0.01)\n",
    "\n",
    "Bias is nearly always initialized to zero using the [torch.nn.init.constant(tensor, val)](http://pytorch.org/docs/master/nn.html#torch.nn.init.constant)\n",
    "\n",
    "To implement an initialization method you must uncomment #net.apply(init_weights) and to select your favorite method you must modify the init_weights function.\n",
    "\n",
    "### Nonlinearity\n",
    "Non-linearity is what makes neural networks universal predictors. Not everything in our universe is related by linearity and therefore we must implement non-linear activations to cope with that. [The most commonly used nonliearities](http://pytorch.org/docs/master/nn.html#non-linear-activations) are listed below. \n",
    "1. ReLU\n",
    "2. Leaky ReLU\n",
    "3. Sigmoid squash the output [0, 1], and are used if your output is binary (not used in the hidden layers)\n",
    "4. Tanh is similar to sigmoid, but squashes in [-1, 1]. It is rarely used any more.\n",
    "5. Softmax normalizes the the output to 1, and is used as output if you have a classification problem\n",
    "\n",
    "Change the current function to another. To do so, you must modify the forward()-method in the Net()-class. \n",
    "\n",
    "### Early stopping\n",
    "Early stopping stops your training when you have reached the best possible model before overfitting. The method saves the model weights at each epoch while constantly monitoring the development of the validation loss. Once the validation loss starts to increase the method will raise a flag. The method will allow for a number of epochs to pass before stopping. The number of epochs are referred to as patience. If the validation loss decreases below the previous global minima before the patience runs out the flag and patience is reset. If a new global minima is not encountered the training is stopped and the weights from the global minima epoch are loaded and defines the final model. \n",
    "\n",
    "To implement early stopping you must set implement=True in the invoke()-function called within train() or train_with_minibatches().\n",
    "\n",
    "### Regularization (optional)\n",
    "Implement either L2 regularization, [dropout](https://pytorch.org/docs/stable/nn.html#dropout-layers) or [batch normalization](https://pytorch.org/docs/stable/nn.html#normalization-layers).\n",
    "\n",
    "### Mix of peptide lengths\n",
    "Now you have hopefully found an architecture that yields a pretty good performance. But of course it is not that simple... One of the issues that occur when working with real data is that ligands can have lengths of 8, 10, or 11 amino acids. In order to accomodate different lengths you need to pad your sequences, so they still fit into the expected tensor. This, however, may mess with the weights of the anchor positions.\n",
    "\n",
    "Try and include 8-9-10-11mers and take a look at how it affects performance. \n",
    "\n",
    "* set MAX_PEP_SEQ_LEN = 11\n",
    "* set ALLELE = 'A0301'\n",
    "\n",
    "#### Performance evaluation\n",
    "Run the notebook and take a look at how the model performs on data partitioned by peptide length. \n",
    "\n",
    "1. What happens to the performance evaluated on 8-10-11mers (excluding 9mers) compared to performance evaluated only on peptides of length 9?\n",
    "\n",
    "Can you explain why we would prefer a good performance on 8-9-10-11mers over a higher performance on only 9mers?\n",
    "\n",
    "## ... continue exercise with notebook CNN-ligand_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "609c9fd0-5d6e-4249-8e96-d59a70403769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f441249-15cd-4a32-aa0f-26a4a1d6f0e7",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ffe395a-cf67-4124-ba10-696e33adcba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions you will re-use\n",
    "# Data-related utility functions\n",
    "def load_blosum(filename):\n",
    "    \"\"\"\n",
    "    Read in BLOSUM values into matrix.\n",
    "    \"\"\"\n",
    "    aa = ['A', 'R', 'N' ,'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V', 'X']\n",
    "    df = pd.read_csv(filename, sep='\\s+', comment='#', index_col=0)\n",
    "    return df.loc[aa, aa]\n",
    "\n",
    "def load_peptide_target(filename):\n",
    "    \"\"\"\n",
    "    Read amino acid sequence of peptides and\n",
    "    corresponding log transformed IC50 binding values from text file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, sep='\\s+', usecols=[0,1], names=['peptide','target'])\n",
    "    return df.sort_values(by='target', ascending=False).reset_index(drop=True)\n",
    "\n",
    "def encode_peptides(X_in, blosum_file, max_pep_len=9):\n",
    "    \"\"\"\n",
    "    Encode AA seq of peptides using BLOSUM50.\n",
    "    Returns a tensor of encoded peptides of shape (1, max_pep_len, n_features) for a single batch\n",
    "    \"\"\"\n",
    "    blosum = load_blosum(blosum_file)\n",
    "    \n",
    "    batch_size = len(X_in)\n",
    "    n_features = len(blosum)\n",
    "    \n",
    "    X_out = np.zeros((batch_size, max_pep_len, n_features), dtype=np.int8)\n",
    "    \n",
    "    for peptide_index, row in X_in.iterrows():\n",
    "        for aa_index in range(len(row.peptide)):\n",
    "            X_out[peptide_index, aa_index] = blosum[ row.peptide[aa_index] ].values\n",
    "            \n",
    "    return X_out, np.expand_dims(X_in.target.values,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "226a7fef-b833-4c37-86b1-b65e20c25db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc. functions\n",
    "def invoke(early_stopping, loss, model, implement=False):\n",
    "    if implement == False:\n",
    "        return False\n",
    "    else:\n",
    "        early_stopping(loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            return True\n",
    "        \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def plot_losses(train_losses, valid_losses, n_epochs):\n",
    "    # Plotting the losses \n",
    "    fig,ax = plt.subplots(1,1, figsize=(9,5))\n",
    "    ax.plot(range(n_epochs), train_losses, label='Train loss', c='b')\n",
    "    ax.plot(range(n_epochs), valid_losses, label='Valid loss', c='m')\n",
    "    ax.legend()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b6ca797-4889-41af-a5d0-8a1b47e96ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model saving and loading functions\n",
    "\n",
    "def save_ffnn_model(filepath, model):\n",
    "    if not filepath.endswith('.pkl'):\n",
    "        filepath = filepath+'.pkl'\n",
    "    with open(filepath, 'wb') as f:\n",
    "        dict_to_save = {'input_size': model.W1.shape[0], 'hidden_size':model.W1.shape[1], 'output_size':model.W2.shape[1],\n",
    "                        'W1': model.W1, 'b1':model.b1, 'W2':model.W2, 'b2':model.b2}\n",
    "        pickle.dump(dict_to_save, f)\n",
    "        print(f'Saved FFNN model at {filepath}')\n",
    "\n",
    "\n",
    "def load_ffnn_model(filepath, model=None):\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        loaded_dict = pickle.load(f)\n",
    "    if model is None:\n",
    "            model = SimpleFFNN(loaded_dict['input_size'], loaded_dict['hidden_size'], loaded_dict['output_size'])\n",
    "    assert (model.W1.shape[0]==loaded_dict['input_size'] and model.W1.shape[1]==loaded_dict['hidden_size'] and model.W2.shape[1]==loaded_dict['output_size']), \\\n",
    "        f\"Model and loaded weights size mismatch!. Provided model has weight of dimensions {model.W1.shape, model.W2.shape} ; Loaded weights have shape {loaded_dict['W1'].shape, loaded_dict['W2'].shape}\"\n",
    "\n",
    "    model.W1 = loaded_dict['W1']\n",
    "    model.b1 = loaded_dict['b1']\n",
    "    model.W2 = loaded_dict['W2']\n",
    "    model.b2 = loaded_dict['b2']\n",
    "    print(f\"Model loaded successfully from {filepath}\\nwith weights [ W1, W2 ] dimensions : {model.W1.shape, model.W2.shape}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44979f18-9c57-400e-8cee-45ec044007bb",
   "metadata": {},
   "source": [
    "# Data loading and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98cb9e0a-3c81-46cf-997e-725ff47d67ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the dataframe ; Peptides have to be *encoded* to BLOSUM matrices\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peptide</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YTYSGLFCV</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ILPVIFLSI</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FMDGKQACV</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FTLIDIWFL</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FVFAWFNGV</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     peptide  target\n",
       "0  YTYSGLFCV     1.0\n",
       "1  ILPVIFLSI     1.0\n",
       "2  FMDGKQACV     1.0\n",
       "3  FTLIDIWFL     1.0\n",
       "4  FVFAWFNGV     1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_datapoints:\n",
      "Train data:\t 2471\n",
      "Valid data:\t 2471\n",
      "Test data:\t 618\n",
      "Maximum peptide length of each data set:\n",
      "Train:\t 9\n",
      "Valid:\t 9\n",
      "Test:\t 9\n"
     ]
    }
   ],
   "source": [
    "# Replace your data paths with the actual paths and desired alleles\n",
    "ALLELE = 'A0201' #'A0301'\n",
    "DATAPATH = '/Users/riwa/Documents/code/22125_algobio_nn/data'\n",
    "blosum_file = f'{DATAPATH}/BLOSUM50'\n",
    "train_data = f'{DATAPATH}/{ALLELE}/train_BA'\n",
    "valid_data = f'{DATAPATH}/{ALLELE}/valid_BA'\n",
    "test_data = f'{DATAPATH}/{ALLELE}/test_BA'\n",
    "\n",
    "# Loading the peptides.\n",
    "train_raw = load_peptide_target(train_data)\n",
    "valid_raw = load_peptide_target(valid_data)\n",
    "test_raw = load_peptide_target(test_data)\n",
    "# \n",
    "print('Preview of the dataframe ; Peptides have to be *encoded* to BLOSUM matrices')\n",
    "display(train_raw.head())\n",
    "\n",
    "print('N_datapoints:')\n",
    "print('Train data:\\t', train_raw.shape[0])\n",
    "print('Valid data:\\t', valid_raw.shape[0])\n",
    "print('Test data:\\t', test_raw.shape[0])\n",
    "\n",
    "print('Maximum peptide length of each data set:')\n",
    "print('Train:\\t',  train_raw['peptide'].apply(len).max())\n",
    "print('Valid:\\t', valid_raw['peptide'].apply(len).max())\n",
    "print('Test:\\t', test_raw['peptide'].apply(len).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9108093e-4d84-4a96-808c-f97b424f8d5c",
   "metadata": {},
   "source": [
    "Peptide encoding : \n",
    "\n",
    "We need to ensure that every peptide shorter than the maximum length `max_len_pep` are *padded* to that length when building the matrices, in order for the dimensions to fit. For example, if our `max_pep_len` is 11, then every peptide of length shorter than 11 must be padded to 11. \n",
    "\n",
    "For example, for a peptide \"GILGFVFTL\", of size 9, and a `max_pep_len` of 11, this effectively means that the peptide is first padded to 11 : \"GILGFVFTLXX\", where \"x\" means a pad, and then converted to a matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c144f09-7d56-46fc-a857-609c9b18f8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2471, 9, 21)\n"
     ]
    }
   ],
   "source": [
    "max_pep_len = train_raw.peptide.apply(len).max()\n",
    "x_train_, y_train_ = encode_peptides(train_raw, blosum_file, max_pep_len)\n",
    "x_valid_, y_valid_ = encode_peptides(valid_raw, blosum_file, max_pep_len)\n",
    "x_test_, y_test_ = encode_peptides(test_raw, blosum_file, max_pep_len)\n",
    "# We now have matrices of shape (N_datapoints, max_pep_len, n_features)\n",
    "print(x_train_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21fb20-f0b9-43be-8ac3-0b6c7ca26f58",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "Now it's your turn. We are using a class structure, where a class contains `methods` needed for the forward pass (that just means functions that belong to a class) and you will code the derivatives and backpropagation separately.\n",
    "\n",
    "For the forward pass, you will need to define the shape of your weight and bias matrices, as well as the actual `forward` pass in terms of matrix multiplications. Additionally, as we are using neural networks, you will also need to code the activation functions (ReLU and Sigmoid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de78ebb7-eb36-4551-8fb9-dbc582e9c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights initialization function.\n",
    "# xavier initialization is technically more stable and preferred \n",
    "# (See slides)\n",
    "def xavier_initialization_normal(input_dim, output_dim):\n",
    "    in_dim, out_dim = shape\n",
    "    shape = (in_dim, out_dim)\n",
    "    stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "    return np.random.normal(0, stddev, size=shape) * 0.1\n",
    "\n",
    "def random_initialization_normal(input_dim, output_dim):\n",
    "    return np.random.randn(input_dim, output_dim) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3a9284a-cf6e-4184-a2db-571aaf4b8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net, x_train, y_train, learning_rate):\n",
    "    \"\"\"\n",
    "    Trains the network for a single epoch, running the forward and backward pass, and compute and return the loss.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    z1, a1, z2, a2,  = net.forward(x_train)\n",
    "    # backward pass\n",
    "    backward(net, x_train, y_train, z1, a1, z2, a2, learning_rate)\n",
    "    loss = np.mean((a2 - y_train) ** 2)\n",
    "    return loss\n",
    "        \n",
    "def eval_network(net, x_valid, y_valid):\n",
    "    \"\"\"\n",
    "    Evaluates the network ; Note that we do not update weights (no backward pass)\n",
    "    \"\"\"\n",
    "    z1, a1, z2, a2 = net.forward(x_valid)\n",
    "    loss = np.mean((a2-y_valid)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224059bc-0dd1-4fb2-94ad-ebbacadc3fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFFNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, initialization_function=xavier_initialization_normal):\n",
    "        # Initialize weights and biases with small random values\n",
    "        self.W1 = initialization_function(XX, XX)\n",
    "        self.b1 = np.zeros(XX)\n",
    "        self.W2 = initialization_function(XX, XX)\n",
    "        self.b2 = np.zeros(XX)\n",
    "        print(self.W1.shape, self.b1.shape, self.W2.shape, self.b2.shape)\n",
    "        \n",
    "    def relu(self, x):\n",
    "        return XX\n",
    "\n",
    "    def sigmoid(self, x): \n",
    "        \"\"\"\n",
    "        The normal version of sigmoid 1 / (1 + np.exp(-x)) is NOT numerically stable\n",
    "        Here we split the case into two for positive and negative inputs\n",
    "        because np.exp(-x) for something negative will quickly overflow if x is a large negative number\n",
    "        \"\"\"\n",
    "        # This is equivalent to : \n",
    "        # if x>=0, then compute (1/(1+np.exp(-x)))\n",
    "        # if x<0: compute (np.exp(x)/(1+np.exp(x))))\n",
    "        return np.where(x >= 0, 1 / (1 + np.exp(-x)), \n",
    "                        np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        zi denotes the output of a hidden layer i\n",
    "        ai denotes the output of an activation function (non-linearity) at layer i\n",
    "        (activations are relu, sigmoid, tanh, etc.)\n",
    "        Use self.function to call a method. for example: self.relu(XX)\n",
    "        \"\"\"\n",
    "\n",
    "        # First layer : Use a relu here for the activation\n",
    "        z1 = XX\n",
    "        a1 = XX\n",
    "        \n",
    "        # Output layer : Use a sigmoid here for the activation\n",
    "        z2 = XX\n",
    "        a2 = XX\n",
    "        \n",
    "        # Return all the intermediate outputs as well because we need them for backpropagation (see slides)\n",
    "        return z1, a1, z2, a2\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return XX\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    For this derivative, it is not necessary to find a numerically stable version.\n",
    "    Just take the base formula and derive it.\n",
    "    \"\"\"\n",
    "    return XX\n",
    "\n",
    "def backward(net, x, y, z1, a1, z2, a2, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Function to backpropagate the gradients from the output to update the weights.\n",
    "    Apply the chain rule and slowly work out the chain derivatives from the output back to the input\n",
    "    Reminder that np.dot(array_1, array_2) and array.T exists to transpose an array for matrix multiplication\n",
    "    \"\"\"\n",
    "    # This assumes that we are computing a MSE as the loss function.\n",
    "    # Look at your slides to compute the gradient backpropagation for a mean-squared error using the chain rule.\n",
    "\n",
    "    # Output layer error ; We used a sigmoid in this layer\n",
    "    error = XX\n",
    "    d_output = XX \n",
    "\n",
    "    # Backpropagate to hidden layer\n",
    "    d_W2 = XX \n",
    "    d_b2 = np.sum(d_output, axis=0, keepdims=True)\n",
    "    d_b2 = d_b2.squeeze() # Squeeze is needed here to make the dimensions fit\n",
    "\n",
    "    # Hidden layer error ; We used a ReLU in this layer!\n",
    "    error_hidden_layer = XX \n",
    "    d_hidden_layer = XX \n",
    "\n",
    "    # Backpropagate to input layer\n",
    "    d_W1 = XX \n",
    "    d_b1 = np.sum(d_hidden_layer, axis=0, keepdims=True)\n",
    "    d_b2 = d_b2.squeeze() # Squeeze is needed here to make the dimensions fit\n",
    "    \n",
    "    # Update weights and biases using gradient descent\n",
    "    net.W1 -= XX\n",
    "    net.b1 -= XX\n",
    "    net.W2 -= XX\n",
    "    net.b2 -= XX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd57a3c-79a9-486e-aaad-79efe15a119d",
   "metadata": {},
   "source": [
    "# Now create a model and run it.\n",
    "\n",
    "Play around with the hyperparameters (number of epochs, learning rate, hidden size) and see what the changes do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff1b8b-0462-4e01-96cb-469fcdd44b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the matrices so they're flat because feed-forward networks are \"one-dimensional\"\n",
    "x_train_ = x_train_.reshape(x_train_.shape[0], -1)\n",
    "x_valid_ = x_valid_.reshape(x_valid_.shape[0], -1)\n",
    "x_test_ = x_test_.reshape(x_test_.shape[0], -1)\n",
    "# Define sizes\n",
    "input_size = x_train_shape[1] # also known as \"n_features\"\n",
    "# Model and training hyperparameters\n",
    "learning_rate = 0.001\n",
    "hidden_units = 50\n",
    "n_epochs = 500\n",
    "output_size = 1\n",
    "# Creating a model instance \n",
    "# You can use either `xavier_initialization_normal` or `random_initialization_normal`\n",
    "# for the initialization_function argument of the class\n",
    "network = SimpleFFNN(input_size, hidden_units, output_size, \n",
    "                     initialization_function=xavier_initialization_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c70c6d1-86f2-4ffe-9deb-bd8ef2ee8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loops\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# Run n_epochs of training\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_network(network, x_train_, y_train_, learning_rate)\n",
    "    valid_loss = eval_network(network, x_valid_, y_valid_)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    # For the first, every 5% of the epochs and last epoch, we print the loss \n",
    "    # to check that the model is properly training. (loss going down)\n",
    "    if (n_epochs >= 10 and epoch % math.ceil(0.05 * n_epochs) == 0) or epoch == 0 or epoch == n_epochs:\n",
    "        print(f\"Epoch {epoch}: \\n\\tTrain Loss:{train_loss:.4f}\\tValid Loss:{valid_loss:.4f}\")\n",
    "\n",
    "# saving the model to a file\n",
    "save_ffnn_model('./saved_ffnn.pkl', model=network)\n",
    "\n",
    "# plotting the losses \n",
    "plot_losses(train_losses, valid_losses, n_epochs)\n",
    "\n",
    "# Reload the model and evaluate it\n",
    "reloaded_network = load_ffnn_model('./saved_ffnn.pkl', model=network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ba8f3-35c4-454c-b910-c0cc9506fc9a",
   "metadata": {},
   "source": [
    "# Evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0da2d-e602-4feb-9ef4-451f7b8e370a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO : Add evaluation / prediction on test data and get the MCC and whate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
